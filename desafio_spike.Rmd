---
title: "Desafio Spike"
author: "Nicolás Sandoval"
date: "05-08-2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(janitor)) install.packages("janitor", repos = "http://cran.us.r-project.org")
if(!require(magrittr)) install.packages("magrittr", repos = "http://cran.us.r-project.org")
if(!require(patchwork)) install.packages("patchwork", repos = "http://cran.us.r-project.org")
if(!require(tidymodels)) install.packages("tidymodels", repos = "http://cran.us.r-project.org")
if(!require(lares)) install.packages("lares", repos = "http://cran.us.r-project.org")
if(!require(modeltime)) install.packages("modeltime", repos = "http://cran.us.r-project.org")
if(!require(timetk)) install.packages("timetk", repos = "http://cran.us.r-project.org")

```

## Carga

Hay 3 archivos que cargar, de precipitaciones, indicadores económicos y del precio de la leche.
Empezamos por los datos de precipitaciones.
### Precipitaciones

```{r, carga de datos precipitaciones}
rain=read_csv("precipitaciones.csv")
visdat::vis_dat(rain)
```
Podemos ver que no hay valores faltantes y todas las columnas fueron importadas como `double`, excepto la columna de fecha.También podemos ver que los datos se encuentran en un formato ancho, con cada fila teniendo más de una observación, lo que no es ideal para trabajar con las herramientas de tidyverse.

```{r, transformacion a formato tidy}
rain_long=rain%>%pivot_longer(!date,names_to = "region", values_to = "mm")
```

Con los datos transformados, podemos fácilmente visualizar las distribuciones, que parecen no tener irregularidades.
```{r distribuciones}
rain_long%>%ggplot(aes(mm))+geom_density()+facet_wrap(~region)
```

Un potencial problema para trabajar con los datos es que las observaciones son mensuales, pero están asignadas temporalmente a un solo día del mes, por lo que es útil crear columnas adicionales de año y mes, que faciliten agrupar los datos sin información diaria.


```{r, carga de datos Precipitaciones}
rain_long%<>%mutate(mes=month(date), anio=year(date), fecha=as_datetime(date))
p1=rain_long%>%
  filter(region=="Biobio")%>%ggplot(aes(date,mm))+geom_line()+labs(title = "Serie de tiempo con fechas diarias")
p2=rain_long%>%
  filter(region=="Biobio")%>%ggplot(aes(mes,mm))+geom_line()+facet_wrap(~anio)+labs(title = "Usando mes y año")
p1+p2

```

Viendo las distribuciones de cada región, parecen no haber valores problemáticos.


```{r}
rain_long%>%ggplot(aes(mm))+geom_density()+facet_wrap(~region)+labs(title="Distribución de precipitaciones por región")

```

### Banco Central

Ahora cargamos los datos del Banco Central.

```{r, carga de datos Banco Central}
banco=read_csv("banco_central.csv")
visdat::vis_dat(banco)
banco%<>%clean_names()
```

El archivo contiene 85 columnas, de las cuales solo un pequeño grupo fue importado como número y gran parte de la base contiene NAs.

```{r}
banco%>%select_if(is.numeric)%>%colnames()
```


Solo las columnas con información de dólares y la de venta de autos fueron leídas como información numérica.  
Mirando los datos, podemos ver que el resto de las columnas con números, tienen puntos como separador de miles y por eso se interpretaron como caracteres.


Podemos crear una función auxiliar que transforme a númerico, asumiendo que si hay comas dentro de las columnas, estas corresponden a decimales.

```{r, limpieza Banco Central}
parse_num=function(x){
    parse_number(x,locale =locale(decimal_mark = ","))
}
```

Antes de aplicar esta función es importante convertir la columna `Periodo` en fecha, para facilitar la aplicación al resto de la base usando `mutate_if`.

```{r, limpieza Banco Central 2}
procesamiento=function(datos_banco){
  datos_banco%>%mutate(fecha=as_datetime(periodo))%>%
  mutate_if(is.character,parse_num)%>%select(-periodo)
}

banco_proc=procesamiento(banco)
visdat::vis_dat(banco_proc)
```

Al aplicar estas transformaciones hay varios datos que entregan errores. El primero está en la columna de fecha (2020-13-01 00:00:00 UTC), donde el mes es 13. Viendo los datos, diciembre de 2020 no se encuentra en la base, por lo que esta columna podría corresponder a ese mes, pero no podemos asegurarlo, por lo que se removió.
Los demás errores están asociados a valores con la letra `a` donde debería haber un NA o un número, sin embargo, parecen simplemente ser errores por lo que se removieron de la base.

```{r, limpieza Banco Central 3}
banco_proc%<>%filter(!is.na(fecha))
```

Después de estas correcciones podemos remover duplicados y ordenar los datos por fecha, para ver si hay alguna relación entre los datos faltantes y la fecha, por ejemplo que alguna variable solo se haya empezado a recolectar después de cierto año.

```{r, limpieza Banco Central 4}
banco_proc%<>%
  distinct()%>%
  arrange(fecha)
visdat::vis_dat(banco_proc)
```

Como se puede ver, ninguna variable tiene valores faltantes después de que aparece el primer valor, a excepción de los últimos meses que se encuentran incompletos, probablemente porque no se había reportado el valor cuando se construyó la base. En este paso además se eliminan las filas duplicadas.


Habiendo completado esto, podemos investigar si los valores dentro de las columnas tienen sentido.

```{r, limpieza Banco Central 5}
p1=banco_proc%>%ggplot(aes(fecha, imacec_industria))+geom_point()+labs(title="Imacec industria por fecha")
p2=banco_proc%>%ggplot(aes(imacec_industria))+geom_density()+labs(title="Distribución de valores de Imacec industria")
p1/p2
```
Tomando una de las variables de Imacec, podemos ver que la distribución es bimodal, lo que usualmente indica algún error.
Además los datos parecieran estar separados en varios niveles que cubren un rango muy amplio de valores y el Imacec se suele reportar como variación porcentual, así que los datos deberían rondar cerca del 0 en vez de llegar a varios millones.

```{r, limpieza Banco Central 6}
imacec_long=banco_proc%>%select(fecha, contains("imacec"))%>%
  pivot_longer(!fecha, names_to = "Tipo", values_to = "Imacec")
```

Si pasamos los datos de Imacec a un nuevo dataframe ordenado, podemos visualizar si esto ocurre en todas las variables.
Dado el amplio rango de valores, es más fácil ver diferencias en escala logarítmica.

```{r, limpieza Banco Central 7}
imacec_long%>%na.omit()%>%
  ggplot(aes(x=fecha, y=Imacec))+geom_point()+scale_y_log10()+facet_wrap(~Tipo)
```

Se pueden apreciar al menos 4 "niveles" en los datos, cuando debería haber solo 1.  

Asumiendo que los datos son válidos pero tienen errores de ingreso, por ejemplo que falte uno o más dígitos, podemos multiplicarlos por un factor para corregir esto.

```{r, limpieza Banco Central 8}
imacec_long%>%
  ggplot(aes(Imacec))+geom_density()+facet_wrap(~Tipo)

imacec_long%<>%na.omit()
max(imacec_long$Imacec)

correccion=function(imacec){
  corte1=10^8.5
  corte2=10^7.5
  corte3=10^6.5
  case_when(imacec>=corte1~imacec/(corte1*10^0.5),
            imacec>=corte2~imacec/(corte2*10^0.5),
            imacec>=corte3~imacec/(corte3*10^0.5),
            TRUE~imacec/1000000)
}
imacec_long%>%
  ggplot(aes(fecha,correccion(Imacec)))+geom_point()+facet_wrap(~Tipo)

imacec_long%<>%mutate(Imacec=correccion(Imacec))
```

No es posible asegurar que los valores sean correctos, pero al menos ahora todos están en el mismo orden de magnitud y parecen tener una tendencia estable. Otra opción hubiera sido definir una regla para identificar y remover outliers, aunque dada las distribuciones de los datos, se perdería gran parte de la información.

Siguiendo el análisis con los datos del PIB.

```{r}
pib_long=banco_proc%>%select(fecha, contains("PIB"))%>%
  pivot_longer(!fecha, names_to = "Tipo", values_to = "PIB")%>%na.omit()
pib_long%>%
  ggplot(aes(x=fecha, y=PIB))+geom_point()+scale_y_log10()+facet_wrap(~Tipo)
pib_long%>%
  ggplot(aes(log(PIB+1)))+geom_density()+facet_wrap(~Tipo,scales = "free")
pib_long%>%
  ggplot(aes(Tipo, log(PIB+1)))+geom_boxplot()
```

Nuevamente nos encontramos con una cantidad importante de valores que parecen ser outliers, sin embargo, al revisar las distribuciones, parecen ser una proporción menor que para el imacec y podemos filtrar usando la desviación estándar de cada variable, descartando valores que estén a más de 2 SDs de la media.

```{r}
resumen_pib=pib_long%>%group_by(Tipo)%>%summarize(mu=mean(PIB), med=median(PIB), s_dev=sd(PIB))
pib_clean=pib_long%>%left_join(resumen_pib, by="Tipo")%>%
  mutate(PIB=ifelse(
    PIB>(mu+2*s_dev)|PIB<(mu-2*s_dev), NA, PIB
  ))%>%distinct()
pib_clean2=pib_long%>%left_join(resumen_pib, by="Tipo")%>%
  mutate(PIB=ifelse(
    PIB>(med+2*s_dev)|PIB<(med-2*s_dev), NA, PIB
  ))%>%distinct()

pib_clean%>%
  ggplot(aes(log(PIB+1)))+geom_density()+facet_wrap(~Tipo,scales = "free")
pib_clean2%>%
  ggplot(aes(log(PIB+1)))+geom_density()+facet_wrap(~Tipo,scales = "free")
```

Revisamos las columnas de índices.

```{r}
ind_long=banco_proc%>%select(fecha, contains("indice"))%>%
  pivot_longer(!fecha, names_to = "Tipo", values_to = "indice")
```

```{r}

ind_long%>%na.omit()%>%
  ggplot(aes(x=fecha, y=indice))+geom_point()+scale_y_log10()+facet_wrap(~Tipo)
```

Podemos ver que ocurre lo mismo que en el Imacec, con los datos separados en varios niveles a diferentes órdendes de magnitud.

```{r}
correccion2=function(indice){
  corte1=10^8.3
  corte2=10^7.3
  corte3=10^6.3
  case_when(indice>=corte1~indice/(corte1*10^0.7),
            indice>=corte2~indice/(corte2*10^0.7),
            indice>=corte3~indice/(corte3*10^0.7),
            TRUE~indice/1000000)
}
ind_long%>%na.omit()%>%
  ggplot(aes(x=fecha, y=correccion2(indice)))+geom_point()+scale_y_log10()+facet_wrap(~Tipo)
ind_long%<>%mutate(indice=correccion2(indice))

ind_long%>%
  ggplot(aes(x=fecha, y=(indice)))+geom_point()+scale_y_log10()+facet_wrap(~Tipo)
```

## Visualización

Creamos las funciones solicitadas.

```{r función de vis, include=TRUE}
vis_lluvia=function(fecha_inicio,fecha_termino, region_solicitada){
  
  rango_base=interval(min(rain_long$date), max(rain_long$date))
  rango=interval(ymd(fecha_inicio), ymd(fecha_termino))
  
  if(region_solicitada%in%rain_long$region)(
    if(rango%within%rango_base) 
      rain_long%>%filter(date%within%rango, region==region_solicitada)%>%
      ggplot(aes(date,mm, group=region))+geom_col()+facet_wrap(~region)
    else "Fechas no se encuentran en la base.")
  else "Región no se encuentra en la base"
}

```

Graficamos las combinaciones de fecha y región solicitadas.

```{r}
p1=vis_lluvia("2000-01-01","2020-01-01", "Biobio")
p2=vis_lluvia("2000-01-01","2020-01-01", "Metropolitana_de_Santiago")
p1/p2
```

Se puede apreciar que hay estaciones secas y humedas, y aunque la estacionalidad es compartida, se ven precipitaciones mayores en la región del Biobio.

Construimos la siguiente función solicitada.

```{r include=TRUE}

vis_lluvia2=function(lista, region_solicitada){
  
rain_long%>%
    mutate(fecha_aux=as.Date(paste0("2000-",mes,"-01")))%>%
    filter(anio%in%lista, region==region_solicitada)%>%
    ggplot(aes(fecha_aux,mm,group=anio, color=as.factor(anio)))+
    geom_line()+
    labs(title = paste("Precipitaciones por año en", region_solicitada),color="Año", x="Mes")+
        scale_x_date(date_labels = "%b", date_breaks = "1 month")+scale_color_viridis_d()

}
años=c(1982, 1992,2002,2012,2019)
vis_lluvia2(años, "Maule")

```

Dado que no es fácil ver las tendencias con todas las líneas en el mismo gráfico, cree otra función que separa los datos por año.

```{r include=TRUE}
vis_lluvia3=function(lista, region_solicitada){
  
rain_long%>%
    mutate(fecha_aux=as.Date(paste0("2000-",mes,"-01")))%>%
    filter(anio%in%lista, region==region_solicitada)%>%
    ggplot(aes(fecha_aux,mm))+
    geom_line()+
    labs(title = paste("Precipitaciones por año en", region_solicitada),color="Año", x="Mes")+
        scale_x_date(date_labels = "%b", date_breaks = "1 month")+facet_wrap(~anio)

}

vis_lluvia3(años, "Maule")

```

Se puede apreciar que los años más recientes son más secos, lo que coincide con los largos periodos de sequía que hemos tenido en Chile, sin embargo, no podemos realmente sacar conclusiones solo mirando 5 años.


Creamos función para graficar PIB.

```{r funcion vis PIB}
vis_pib=function(pib1, pib2){
  pib_clean%>%filter(Tipo==pib1|Tipo==pib2, fecha>=as_datetime("2013-01-01"))%>%
    ggplot(aes(fecha,PIB, color=Tipo))+geom_line()
}

vis_pib("PIB_Agropecuario_silvicola","PIB_Servicios_financieros")
```


Dado que parte de los datos presentaban errores, el PIB de servicio financieros se ve incompleto, aunque siempre al alza, mientras que el agropecuario tiene estacionalidad y sin una tendencia clara al alza. No parece haber una relación entre ambas series.

### Tratamiento y creación de variables
Para determinar qué variables escoger para el modelo es importante determinar tanto qué variables tienen alta correlación con nuestra variable a predecir y también qué variables aportan información distinta al modelo. Es común tener variables de alta correlación con nuestro outcome que tienen a la vez alta correlación entre ellas. 

```{r importar leche}
leche=read_csv("precio_leche.csv")%>%clean_names()
levels(as.factor(leche$Mes))
reemplazo_mes=function(mes){
  case_when(mes=="Abr"~"04", mes=="Ago"~"08", mes=="Dic"~"12",mes=="Ene"~"01", mes=="Feb"~"02", mes=="Jul"~"07", mes=="Jun"~"06", mes=="Mar"~"03", mes=="May"~"05", mes=="Nov"~"11", mes=="Oct"~"10" ,mes=="Sep"~"09")}
leche%<>%mutate(mes=map_chr(mes,reemplazo_mes),fecha=as_datetime(paste0(anio,"-",mes,"-01")),trimestre=quarter(fecha))%>% mutate(precio_sig=lead(precio_leche),mes=parse_number(mes))

```

Dado que gran parte de las variables económicas no tienen datos antes de 1996, se descaron las filas previas.

```{r}
pib_wide=pib_long%>%pivot_wider(names_from = "Tipo", values_from = "PIB")
imacec_wide=imacec_long%>%pivot_wider(names_from = "Tipo", values_from = "Imacec")
ind_wide=ind_long%>%na.omit()%>%pivot_wider(names_from = "Tipo", values_from = "indice")
rain_final=rain_long%>%group_by(mes, anio)%>%
  summarize(mm_total=sum(mm))

base_combinada=leche%>%
  left_join(rain_final, by=c("anio", "mes"))%>%
  left_join(imacec_wide, by="fecha")%>%
  left_join(pib_wide, by="fecha")%>%
  left_join(ind_wide, by="fecha")
  
base_combinada%<>%
  filter(fecha>=as_datetime("1996-01-01"))

#falta incluir datos de precios
ts_split = base_combinada%>%time_series_split(date_var = fecha, assess = "365 days", cumulative = TRUE)
split_df=ts_split %>%
  tk_time_series_cv_plan()
```

Por ejemplo, construyendo una pequeña base combinada, podemos usar la función corr_var, para ver la correlación de todas las variables contra la variable de interés (precio de la leche del mes siguiente). Esto se debe hacer después de separar nuestros datos en un set de entrenamiento y uno de prueba, para evitar influenciar la modelación con información de nuestro set de prueba. Escogí usar el último año de datos como set de prueba, para mantener la estacionalidad de los datos.

```{r}
train_set=split_df%>%filter(.key=="training")%>%select(-.key, -.id)
test_set=split_df%>%filter(.key=="testing")%>%select(-.key, -.id)

cor_matrix= train_set%>%
  dplyr::select(where(is.numeric)) %>% 
  na.omit()

corr_var(cor_matrix,precio_sig, top=20)
corr_var(cor_matrix,imacec_industria, top=20)
corr_var(cor_matrix,pib_mineria, top=20)

```

Después podemos revisar la correlación entre variables predictoras y así no considerar variables de muy alta correlación con las que ya fueron incluidas.

Un ejemplo claro de variables correlacionadas son las temporales, los trimestres siempre están asociados a los mismos meses, por lo que incluir ambas como predictores probablemente no va a ser mejor que incluir solo una. Varias variables económicas también tiene alta correlación, como el Imacec empalmado y el Imacec a costo de factores (cor=0.997).

```{r}
corr_cross(cor_matrix, max_pvalue = 0.05,top = 30)
```

## Modelos

El primer modelo a considerar es uno de regresión lineal regularizada, y la penalización se calibró usando cross-validation (k=10).
Para medir el desempeño se seleccionó el RMSE, de modo que tanto sobreestimar como subestimar el precio tenga el mismo peso. Además esta medida nos entrega el error en las mismas unidades que la predicción.

```{r}
folds=vfold_cv(train_set)

leche_rec=recipe(precio_sig~
                   precio_leche+
                   imacec_industria+
                   pib_mineria+
                   pib_refinacion_de_petroleo+
                   indice_de_produccion_industrial_electricidad_gas_y_agua+
                   pib_alimentos+
                   mm_total,
                 data=train_set)%>%
  step_normalize(all_numeric_predictors())%>%
  step_dummy(all_nominal())%>%
  step_impute_mean(all_numeric_predictors())
summary(leche_rec)
tidy(leche_rec)
leche_rec%>%prep()

met=metric_set(yardstick::rmse)
grid_control=control_grid(extract=extract_model,save_pred = TRUE, save_workflow = TRUE)


lm_model = linear_reg(penalty = tune()) %>% set_engine("glmnet")

lm_wflow = 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(leche_rec)
lm_tune=lm_wflow%>%  tune_grid(resamples=folds,metrics=met, grid=tibble(penalty=seq(0,10,0.5)))

lm_tune%>%collect_metrics()%>%arrange(mean)

autoplot(lm_tune)

lm_fit=lm_wflow %>%
  finalize_workflow(select_best(lm_tune)) %>%
  fit(train_set)

lm_fit%>%augment(test_set) %>%
  yardstick::rmse(precio_sig, .pred)
```

El mejor resultado se obtuvo con un lambda de regularización de 0, y el RMSE fue de 11.1.

Luego probamos un modelo menos interpretable (gradient boosted trees), pero que muchas veces entrega mejores resultados predictivos, sobre todo cuando hay relaciones no lineales entre variables. El RMSE obtenido fue de 16.3, algo mayor que el del modelo lineal.
```{r}
bt_model =
  boost_tree(mode="regression",
            mtry = tune(),
             trees = tune(),
             learn_rate = tune()
           #  ,learn_rate = tune(),tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune()
             )%>%
  set_engine("xgboost")

bt_wflow = 
  workflow() %>% 
  add_model(bt_model) %>% 
  add_recipe(leche_rec)

bt_tune = bt_wflow %>%
  tune_grid(folds,
            metrics = met,
            control = grid_control,
            grid =crossing(mtry = c(4,6),
                            trees = seq(200,1200, 100), learn_rate=c(0.01)))

autoplot(bt_tune)

bt_tune %>%
  collect_metrics() %>%
  arrange(mean)

bt_fit = bt_wflow %>%
  finalize_workflow(select_best(bt_tune)) %>%
  fit(train_set)

bt_fit%>%augment(test_set) %>%
  yardstick::rmse(precio_sig, .pred)

```

El RMSE obtenido fue de 16.1, algo mayor que el del modelo lineal.

• ¿Qué datos adicionales te gustaría tener?¿Qué datos son necesarios para que este
modelo funcione/mejore las métricas?
• ¿Cómo evalúas el resultado del modelo?¿Qué métricas tiene sentido mirar?
• ¿Para qué aplicaciones puede servir un modelo de este tipo? En particular, ¿Cómo
podría ayudar a combatir el cambio climático?